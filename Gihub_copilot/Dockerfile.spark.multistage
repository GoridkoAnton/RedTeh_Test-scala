# Multi-stage Dockerfile: build fat-jar with sbt installed manually (builder),
# then runtime image with Spark (runner).

# -----------------------
# Stage 1: builder
# -----------------------
FROM eclipse-temurin:11-jdk AS builder

WORKDIR /src

# Install tools required to add sbt repo and build
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      curl gnupg apt-transport-https ca-certificates unzip git && \
    rm -rf /var/lib/apt/lists/*

# Add scala-sbt repo and its key, then install sbt
# Note: apt-key may be deprecated on some systems but still works in many images.
RUN curl -sL "https://keyserver.ubuntu.com/pks/lookup?op=get&search=0x99E82A75642AC823" | apt-key add - || true && \
    echo "deb https://repo.scala-sbt.org/scalasbt/debian all main" > /etc/apt/sources.list.d/sbt.list && \
    apt-get update && \
    apt-get install -y --no-install-recommends sbt && \
    rm -rf /var/lib/apt/lists/*

# Copy only necessary files to leverage Docker layer cache
COPY build.sbt /src/
COPY project /src/project
COPY project/plugins.sbt /src/project/plugins.sbt
COPY src /src/src

# Build fat-jar (sbt-assembly must be configured in project/plugins.sbt)
# Copy result to /out/job.jar (try assembly-pattern, fallback to any jar)
RUN sbt -batch -no-colors clean assembly && \
    mkdir -p /out && \
    sh -c 'cp target/scala-2.12/*assembly*.jar /out/job.jar || cp target/scala-2.12/*.jar /out/job.jar'

# -----------------------
# Stage 2: runtime with Spark
# -----------------------
FROM eclipse-temurin:11-jre

ENV SPARK_VERSION=3.4.2
ENV HADOOP_VERSION=3

RUN apt-get update && \
    apt-get install -y --no-install-recommends wget tar ca-certificates && \
    rm -rf /var/lib/apt/lists/*

# Download prebuilt Spark
RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /opt/ && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /opt/spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin

WORKDIR /app

# Copy built jar from builder stage
COPY --from=builder /out/job.jar /app/job.jar

# spark-submit entrypoint; CLI args passed through
ENTRYPOINT ["bash", "-lc", "/opt/spark/bin/spark-submit --master local[4] --class com.example.SmallFilesAndCompact /app/job.jar $@"]