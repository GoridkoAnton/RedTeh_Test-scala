# Stage 1: build fat-jar with sbt (builder)
# Использует sbt образ с JDK11. При необходимости переключите тег на доступный в вашем окружении.
FROM sbt:1.8.22-jdk11 AS builder

WORKDIR /src

# Копируем только необходимые файлы для быстрой кэшируемой сборки
COPY build.sbt /src/
COPY project /src/project
COPY project/plugins.sbt /src/project/plugins.sbt
COPY src /src/src

# Собираем fat-jar (sbt-assembly должен быть в project/plugins.sbt)
# Копируем результат в /out/job.jar (попытка по маске assembly, fallback - любой jar)
RUN sbt -batch -no-colors clean assembly && \
    mkdir -p /out && \
    sh -c 'cp target/scala-2.12/*assembly*.jar /out/job.jar || cp target/scala-2.12/*.jar /out/job.jar'

# Stage 2: runtime with Spark
FROM eclipse-temurin:11-jre

ENV SPARK_VERSION=3.4.2
ENV HADOOP_VERSION=3

RUN apt-get update && \
    apt-get install -y --no-install-recommends wget tar ca-certificates && \
    rm -rf /var/lib/apt/lists/*

# Download prebuilt Spark
RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /opt/ && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /opt/spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin

WORKDIR /app

# Копируем собранный jar из builder stage
COPY --from=builder /out/job.jar /app/job.jar

# Запуск spark-submit; аргументы передаются в конец команды
ENTRYPOINT ["bash", "-lc", "/opt/spark/bin/spark-submit --master local[4] --class com.example.SmallFilesAndCompact /app/job.jar $@"]