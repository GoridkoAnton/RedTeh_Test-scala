# Multi-stage Dockerfile: build fat-jar with sbt-extras (builder),
# then runtime image with Spark (runner).

# -----------------------
# Stage 1: builder
# -----------------------
FROM eclipse-temurin:11-jdk AS builder

WORKDIR /src

# Install basic tools and dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      curl ca-certificates git unzip tar gzip && \
    rm -rf /var/lib/apt/lists/*

# Install sbt using sbt-extras wrapper (robust: downloads launcher dynamically)
# This avoids relying on a specific GitHub release tarball name.
RUN curl -fSL https://raw.githubusercontent.com/paulp/sbt-extras/master/sbt -o /usr/local/bin/sbt && \
    chmod +x /usr/local/bin/sbt

# Ensure sbt will consult central repositories (helps in restricted envs)
RUN mkdir -p /root/.sbt && printf '[repositories]\nlocal\nmaven-central: https://repo1.maven.org/maven2/\nscalasbt-maven: https://repo.scala-sbt.org/scalasbt/maven-releases/\n' > /root/.sbt/repositories

# Copy project files (use COPY . /src if you prefer)
COPY build.sbt /src/
COPY project /src/project
COPY project/plugins.sbt /src/project/plugins.sbt
COPY project/build.properties /src/project/build.properties
COPY src /src/src

# Build fat-jar (sbt-assembly must be configured in project/plugins.sbt)
# Try to copy assembly jar to /out/job.jar; fallback to any jar if assembly name differs
RUN sbt -batch -no-colors clean assembly && \
    mkdir -p /out && \
    sh -c 'cp target/scala-2.12/*assembly*.jar /out/job.jar || cp target/scala-2.12/*.jar /out/job.jar'

# -----------------------
# Stage 2: runtime with Spark
# -----------------------
FROM eclipse-temurin:11-jre

ENV SPARK_VERSION=3.4.2
ENV HADOOP_VERSION=3

RUN apt-get update && \
    apt-get install -y --no-install-recommends wget tar ca-certificates gzip && \
    rm -rf /var/lib/apt/lists/*

# Download prebuilt Spark
RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /opt/ && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /opt/spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin

WORKDIR /app

# Copy built jar from builder stage
COPY --from=builder /out/job.jar /app/job.jar

# spark-submit entrypoint; CLI args passed through
ENTRYPOINT ["bash", "-lc", "/opt/spark/bin/spark-submit --master local[4] --class com.example.SmallFilesAndCompact /app/job.jar $@"]