# Multi-stage Dockerfile: build fat-jar inside the container using sbt,
# then runtime image with Spark. Ensures PostgreSQL JDBC driver is present
# in $SPARK_HOME/jars and uses a small exec-form entrypoint wrapper that
# launches spark-submit with forwarded arguments.

# -----------------------
# Stage 1: builder
# -----------------------
FROM eclipse-temurin:11-jdk AS builder

WORKDIR /src

# Install tools
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
      curl ca-certificates git unzip tar gzip coreutils wget && \
    rm -rf /var/lib/apt/lists/*

# Install sbt wrapper (sbt-extras)
RUN curl -fsSL https://raw.githubusercontent.com/paulp/sbt-extras/master/sbt -o /usr/local/bin/sbt && \
    chmod +x /usr/local/bin/sbt

# Ensure sbt will consult central repositories
RUN mkdir -p /root/.sbt && printf '[repositories]\nlocal\nmaven-central: https://repo1.maven.org/maven2/\nscalasbt-maven: https://repo.scala-sbt.org/scalasbt/maven-releases/\n' > /root/.sbt/repositories

# Prefetch sbt launcher for sbt 1.8.2
ENV SBT_VER=1.8.2
RUN mkdir -p /root/.sbt/launchers/${SBT_VER} && \
    curl -fSL -o /root/.sbt/launchers/${SBT_VER}/sbt-launch.jar \
      https://repo1.maven.org/maven2/org/scala-sbt/sbt-launch/${SBT_VER}/sbt-launch-${SBT_VER}.jar && \
    (cd /root/.sbt/launchers/${SBT_VER} && md5sum sbt-launch.jar | awk '{print $1}' > sbt-launch.jar.md5)

# Copy full project (simpler and less error-prone)
COPY . /src
WORKDIR /src

# Build fat-jar (sbt-assembly must be configured in project/plugins.sbt)
RUN sbt -batch -no-colors clean assembly && \
    mkdir -p /out && \
    sh -c 'cp target/scala-2.12/*assembly*.jar /out/job.jar || cp target/scala-2.12/*.jar /out/job.jar'

# -----------------------
# Stage 2: runtime with Spark
# -----------------------
FROM eclipse-temurin:11-jre

ENV SPARK_VERSION=3.4.2
ENV HADOOP_VERSION=3
ENV SPARK_HOME=/opt/spark
ENV PATH=$PATH:$SPARK_HOME/bin

# Default driver memory (can be overridden at runtime via ENV)
ENV SPARK_DRIVER_MEMORY=6g

RUN apt-get update && \
    apt-get install -y --no-install-recommends wget tar ca-certificates gzip && \
    rm -rf /var/lib/apt/lists/*

# Download prebuilt Spark
RUN wget -q https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /opt/ && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} /opt/spark && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Ensure Spark jars dir exists then download PostgreSQL JDBC driver into Spark jars
RUN mkdir -p /opt/spark/jars && \
    wget -q -O /opt/spark/jars/postgresql-42.6.0.jar \
      https://repo1.maven.org/maven2/org/postgresql/postgresql/42.6.0/postgresql-42.6.0.jar

WORKDIR /app

# Copy built jar from builder stage
COPY --from=builder /out/job.jar /app/job.jar

# Copy entrypoint wrapper and make executable
# (entrypoint script should be present in the build context at app/entrypoint.sh)
COPY app/entrypoint.sh /app/entrypoint.sh
RUN chmod +x /app/entrypoint.sh

# Use exec-form ENTRYPOINT so Docker args are appended and env is preserved
ENTRYPOINT ["/app/entrypoint.sh"]
CMD []